{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stan081/AML_CourseWork/blob/main/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJm-ZGRPSthM"
      },
      "source": [
        "*PREPROCESSING*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSmsDP5HSthN"
      },
      "source": [
        "This notebook is dedicated to preprocessing the datasets used for this project. It contains the necessary functions to ensure the datasets are ready for training all models and a final output size of **(axb)**. The notebook is divided into two parts.\n",
        "Part one is for preprocessing of dataset 1.\n",
        "Part two is for preprocessing of dataset 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv12Oyc0SthO"
      },
      "source": [
        "**Dataset One**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALtEpFoUSthP"
      },
      "source": [
        "**Dataset Two**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do85XhPVSthP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import shutil\n",
        "import math\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDutHc5JSthQ"
      },
      "outputs": [],
      "source": [
        "# Original Image size in Inria Aerial Image Dataset\n",
        "master_size = 5000\n",
        "\n",
        "# Desired tile size\n",
        "image_size = 384\n",
        "\n",
        "# Overlap percentage\n",
        "overlap = 0.3\n",
        "\n",
        "# working directory\n",
        "root_folder = os.getcwd()\n",
        "\n",
        "# Original dataset folder\n",
        "data_folder = os.path.join(root_folder, 'Downloads/AerialImageDataset_small/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j22dHJq3SthR"
      },
      "outputs": [],
      "source": [
        "# Original dataset folders\n",
        "src_train_folder = os.path.join(data_folder, 'train', 'images')\n",
        "src_train_folder_gt = os.path.join(data_folder, 'train', 'gt')\n",
        "src_test_folder = os.path.join(data_folder, 'test', 'images')\n",
        "\n",
        "print('Training images address    = ', src_train_folder)\n",
        "print('Training gt images address = ', src_train_folder_gt)\n",
        "print('Testing images address     = ', src_test_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO5EanUrSthR"
      },
      "outputs": [],
      "source": [
        "# Training set file names\n",
        "src_train_images = os.listdir(src_train_folder)\n",
        "print(src_train_images)\n",
        "print('Total number of main images = ',len(src_train_images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6evrb_VWSthS"
      },
      "outputs": [],
      "source": [
        "# Testing set file names\n",
        "src_test_images = os.listdir(src_test_folder)\n",
        "print(src_test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpP5ksTDSthS"
      },
      "outputs": [],
      "source": [
        "# set destination folders\n",
        "\n",
        "train_folder_root = os.path.join(data_folder, 'train_{}x{}'.format(image_size, image_size))\n",
        "train_folder = os.path.join(train_folder_root, 'images')\n",
        "train_folder_gt = os.path.join(train_folder_root, 'gt')\n",
        "\n",
        "print(train_folder_root)\n",
        "print(train_folder)\n",
        "print(train_folder_gt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFQ3ruNISthT"
      },
      "outputs": [],
      "source": [
        "# Creating destination folder root\n",
        "if not os.path.exists(train_folder_root):\n",
        "    os.makedirs(train_folder_root)\n",
        "\n",
        "# Creating destination training folder\n",
        "if not os.path.exists(train_folder):\n",
        "    os.makedirs(train_folder)\n",
        "else:\n",
        "    shutil.rmtree(train_folder)\n",
        "\n",
        "# Creating destination training gt folder\n",
        "if not os.path.exists(train_folder_gt):\n",
        "    os.makedirs(train_folder_gt)\n",
        "else:\n",
        "    shutil.rmtree(train_folder_gt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DudVCLDlSthT"
      },
      "outputs": [],
      "source": [
        "# find the number of tiles with 0.3 percent overlap\n",
        "#  (5000)/(384-(384*0.3))\n",
        "\n",
        "count = math.ceil((master_size / (image_size-(image_size*overlap))))\n",
        "\n",
        "# find the number of pixels for each step\n",
        "step = (master_size - image_size * overlap) / count\n",
        "print('count =', count, ', step =', step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVDj1Xh-SthU"
      },
      "outputs": [],
      "source": [
        "# slicing the main images and ground truths into tiles of 384 * 384 pixels\n",
        "# each 5000 * 5000 source will be divided into 19 * 19 = 361 tiles\n",
        "\n",
        "for filename in src_train_images:\n",
        "    print(filename)\n",
        "    master_img = cv.imread(os.path.join(src_train_folder, filename))\n",
        "    master_img_gt = cv.imread(os.path.join(src_train_folder_gt, filename))\n",
        "\n",
        "    for i in range(count):\n",
        "        if i < count - 1:\n",
        "            y = round(i * step)\n",
        "        else:\n",
        "            y = master_size - image_size # last tile of the column\n",
        "\n",
        "        for j in range(count):\n",
        "            if j < count - 1:\n",
        "                x = round(j * step)\n",
        "            else:\n",
        "                x = master_size - image_size # last tile of the row\n",
        "\n",
        "\n",
        "# Slice the main image based on (x,y)  first go ====>>> img = master_img[0:0+384, 0:0+384] = img = master_img[0:384,0:384]\n",
        "            img = master_img[y:y+image_size, x:x+image_size]\n",
        "            img_gt = master_img_gt[y:y+image_size, x:x+image_size]\n",
        "\n",
        "# write the slice (tile) into disk\n",
        "# filename[:-4] ?? to get rid of file extension in original filename (ex: austin1.tif ==>>>  austin1 )\n",
        "\n",
        "            img_fname = '{}_{}_{}.{}'.format(filename[:-4], i, j, 'jpg')\n",
        "            img_gt_fname = '{}_{}_{}.{}'.format(filename[:-4], i, j, 'png')\n",
        "            cv.imwrite(os.path.join(train_folder, img_fname), img)\n",
        "            cv.imwrite(os.path.join(train_folder_gt, img_gt_fname), img_gt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nt6O6iSSthU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5ZKHaPuSthU"
      },
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}